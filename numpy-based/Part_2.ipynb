{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Q-learning with Neural Network approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/nn_v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The feedforward equations are:\n",
    "\\begin{equation}\n",
    "    \\color{blue}q = w_1\\color{blue}{x_1} + w_2\\color{blue}{x_2} + w_3\\color{blue}{x_3} + b = \\left[\\begin{array}{ccc}w_1&w_2&w_3\\end{array}\\right]\\left[\\color{blue}{\\begin{array}{c}x_1\\\\\n",
    "    x_2\\\\\n",
    "    x_3\\end{array}}\\right] + b = W\\color{blue}X + b\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{blue}z = f(\\color{blue}q) = f(W\\color{blue}X + b)\n",
    "\\end{equation}\n",
    "\n",
    "Consider training the network with mini batches of size $m$, then, the sizes of the diferent matrices become:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{blue}X \\rightarrow d\\times m\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    W \\rightarrow 1\\times d\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{blue}q \\rightarrow 1\\times m\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{blue}z \\rightarrow 1\\times m\n",
    "\\end{equation}\n",
    "\n",
    "where $d$ denotes the dimension of the input space, which in our example is $d=3$.\n",
    "\n",
    "### Backward pass\n",
    "Once the forward pass is complete, we can compute the prediction error as (here we use the MSE as an example):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}L = \\frac{1}{2m}(\\hat{z} - \\color{blue}z)^2 \\rightarrow 1\\times m\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{z}$ denotes the desired output or target. The gradient of $\\color{red}L$ w.r.t $\\color{blue}z$ becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_zL} = -\\frac{1}{m}(\\hat{z} - \\color{blue}z) \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\n",
    "this gradient is the one that is going to be backpropagated. If we denote:\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(\\color{blue}q) = \\frac{df(\\color{blue}{q})}{dq}\\bigg|_{\\color{blue}q=W^T\\color{blue}X + b} \\rightarrow 1\\times m\n",
    "\\end{equation}\n",
    "\n",
    "the gradient of $\\color{red}L$ w.r.t $\\color{blue}q$ is (here $*$ denotes elementwise multiplication):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_qL} = f'(\\color{blue}q)*\\color{red}{\\nabla_zL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\n",
    "Following the same idea the other backpropagation equations are:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{x_1}L} = w_1\\color{red}{\\nabla_qL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{x_2}L} = w_2\\color{red}{\\nabla_qL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{x_3}L} = w_3\\color{red}{\\nabla_qL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{w_1}L} = \\color{blue}{x_1}\\color{red}{\\nabla_qL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{w_2}L} = \\color{blue}{x_2}\\color{red}{\\nabla_qL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{w_3}L} = \\color{blue}{x_3}\\color{red}{\\nabla_qL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_bL} = \\color{red}{\\nabla_qL} \\rightarrow 1\\times m \n",
    "\\end{equation}\n",
    "\n",
    "In matrix form these equations are:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_XL} = W^T\\color{red}{\\nabla_qL} \\rightarrow d\\times m  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_WL} = \\color{red}{\\nabla_qL}\\color{blue}X^T \\rightarrow 1\\times d \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_bL} = \\sum\\color{red}{\\nabla_qL} \\rightarrow 1\\times 1  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from envs.environments import grid_world\n",
    "from utilities import plot_policies\n",
    "from agents.Qlearning import nn_Qlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two layers neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_layers_nn():\n",
    "    def __init__(self,input_dim,hidden1_dim,hidden2_dim,output_dim,learning_rate=0.001):\n",
    "        self._in_dim = input_dim\n",
    "        self._h1_dim = hidden1_dim\n",
    "        self._h2_dim = hidden2_dim\n",
    "        self._out_dim = output_dim\n",
    "        self._alpha = learning_rate\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        #Fan in (XAVIER INITIALIZATION)\n",
    "        fin1 = 1.0/np.sqrt(self._in_dim)\n",
    "        fin2 = 1.0/np.sqrt(self._h1_dim)\n",
    "        fin3 = 1.0/np.sqrt(self._h2_dim)\n",
    "\n",
    "        #Weights\n",
    "        self._W01 = 2.0*fin1*np.random.random((self._h1_dim,self._in_dim))-fin1    # Shape: (h1 x d)\n",
    "        self._W12 = 2.0*fin2*np.random.random((self._h2_dim,self._h1_dim))-fin2    # Shape: (h2 x h1)\n",
    "        self._W23 = 2.0*fin3*np.random.random((self._out_dim,self._h2_dim))-fin3   # Shape: (out x h2)\n",
    "\n",
    "        #Biases\n",
    "        self._B1 = 2.0*fin1*np.random.random((self._h1_dim,1))-fin1                # Shape: (h1 x 1)\n",
    "        self._B2 = 2.0*fin2*np.random.random((self._h2_dim,1))-fin2                # Shape: (h2 x 1)\n",
    "        self._B3 = 2.0*fin3*np.random.random((self._out_dim,1))-fin3               # Shape: (out x 1)\n",
    "  \n",
    "    def ReLU(self,x,deriv=False):\n",
    "        if not deriv:\n",
    "            return np.maximum(x,0.0)\n",
    "        else:\n",
    "            return np.maximum(np.sign(x),0.0)\n",
    "        \n",
    "    def linear(self,x,deriv=False):\n",
    "        if not deriv:\n",
    "            return x\n",
    "        else:\n",
    "            return 1.0\n",
    "    \n",
    "    def forward_pass(self,input_data):\n",
    "        assert input_data.shape[0]==self._in_dim, \"Input must have shape: (in x batch)\"        \n",
    "        self._layer0 = input_data                                           # Shape: (d x m)\n",
    "        self._layer1 = self.ReLU(np.dot(self._W01,self._layer0)+self._B1)   # Shape: (h1 x m)\n",
    "        self._layer2 = self.ReLU(np.dot(self._W12,self._layer1)+self._B2)   # Shape: (h2 x m)\n",
    "        self._layer3 = self.linear(np.dot(self._W23,self._layer2)+self._B3) # Shape: (out x m)\n",
    "        return self._layer3\n",
    "    \n",
    "    def backward_pass(self,target):\n",
    "        #LOSS\n",
    "        assert target.shape[0]==self._out_dim, \"Target must have shape: (out x batch)\"\n",
    "        batch_size = target.shape[1]\n",
    "        loss = np.square(target-self._layer3)/(2.0*batch_size)   # Shape: (out x m)\n",
    "        grad_loss = -(target-self._layer3)/(batch_size)          # Shape: (out x m)\n",
    "\n",
    "        #LAYER 3\n",
    "        grad_in_layer3 = grad_loss                                        # Shape: (out x m)\n",
    "        grad_q_layer3 = self.linear(self._layer3,True)*grad_in_layer3     # Shape: (out x m)\n",
    "        grad_W23 = np.dot(grad_q_layer3,self._layer2.T)                   # Shape: (out x h2)\n",
    "        grad_B3 = np.sum(grad_q_layer3,axis=1).reshape(self._out_dim,1)   # Shape: (out x 1)\n",
    "\n",
    "\n",
    "        #LAYER 2\n",
    "        grad_in_layer2 = np.dot(self._W23.T,grad_q_layer3)               # Shape: (h2 x m)\n",
    "        grad_q_layer2 = self.ReLU(self._layer2,True)*grad_in_layer2      # Shape: (h2 x m)\n",
    "        grad_W12 = np.dot(grad_q_layer2,self._layer1.T)                  # Shape: (h2 x h1)\n",
    "        grad_B2 = np.sum(grad_q_layer2,axis=1).reshape(self._h2_dim,1)   # Shape: (h1 x 1)\n",
    "\n",
    "\n",
    "        #LAYER 1\n",
    "        grad_in_layer1 = np.dot(self._W12.T,grad_q_layer2)               # Shape: (h1 x m)\n",
    "        grad_q_layer1 = self.ReLU(self._layer1,True)*grad_in_layer1      # Shape: (h1 x m)\n",
    "        grad_W01 = np.dot(grad_q_layer1,self._layer0.T)                  # Shape: (h1 x d)\n",
    "        grad_B1 = np.sum(grad_q_layer1,axis=1).reshape(self._h1_dim,1)   # Shape: (h1 x 1)\n",
    "\n",
    "        \n",
    "        # Update weights (simple gradient descent)\n",
    "        self._W01 -= self._alpha*grad_W01\n",
    "        self._W12 -= self._alpha*grad_W12\n",
    "        self._W23 -= self._alpha*grad_W23\n",
    "        self._B1 -= self._alpha*grad_B1\n",
    "        self._B2 -= self._alpha*grad_B2\n",
    "        self._B3 -= self._alpha*grad_B3\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting on toy data (to test NN implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1],[1,0,0]]).T\n",
    "Y = np.array([[1],[3],[5],[7],[4]]).T\n",
    "nn = two_layers_nn(3,20,20,1,learning_rate=0.1)\n",
    "Ntrain = 1000\n",
    "for iteration in range(Ntrain+1):\n",
    "    nn.forward_pass(X)\n",
    "    loss = nn.backward_pass(Y)\n",
    "    if(iteration%(Ntrain/10)== 0):\n",
    "        print('It. ',iteration,' Loss: ',np.mean(loss))\n",
    "\n",
    "predicted = nn.forward_pass(X)\n",
    "print('Predicted: ',np.round(predicted,3)[0])\n",
    "print('Correct: ',Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning in the gridworld environment with NN approximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(env,agent,max_episodes,max_steps):\n",
    "    np.random.seed(123) # Fixed random seed\n",
    "    for episode in range(max_episodes+1):\n",
    "        env.reset(random_initial=True)\n",
    "        state = np.array(env.observe_coordinates()).reshape(2,1)\n",
    "        done = False\n",
    "        step = 1\n",
    "        while (not done and step <= max_steps):\n",
    "            action = agent.epsilon_greedy(state)\n",
    "            _,reward,done = env.step(action)\n",
    "            next_state = np.array(env.observe_coordinates()).reshape(2,1)\n",
    "            agent.train(state,action,next_state,reward,done)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        agent.epsilon_decay(rate=0.99)\n",
    "        if(episode%(max_episodes/10)==0):\n",
    "            print('Episode: ',episode,' Steps: ',step,' Exploration: ',np.round(agent._epsilon,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAACl5JREFUeJzt3fGr3XUdx/HXq+v0NjUFZ2G7ow2ygUQ5GYuxENqwZor2Qz9soJAE5ydNKRDtt/4BsR9CGFMTXEpNBZHlklRMWMu7ucztTlnD2B3atsLURZub736438XVFud7dj6f+z333fMBF++598vZ+3B5+v2e7/3e78cRIQA5farrAQDUQ+BAYgQOJEbgQGIEDiRG4EBiBA4kRuBAYgQOJHZejSc93xfEuC6s8dQAJP1Lx3UyTrjfdlUCH9eF+prX1XhqAJJ2xm9bbcchOpAYgQOJETiQGIEDiRE4kBiBA4kROJAYgQOJtQrc9nrbb9g+YPue2kMBKKNv4LbHJP1M0vWSrpK00fZVtQcDMLw2e/BVkg5ExMGIOCnpcUk31x0LQAltAl8s6dCsx9PN1wCMuGJ/bGK7J6knSeNaWOppAQyhzR78sKQlsx5PNF/7mIjYFBErI2LlAl1Qaj4AQ2gT+CuSrrS9zPb5kjZIerruWABK6HuIHhGnbN8uabukMUkPRcTe6pMBGFqr9+ARsU3StsqzACiMK9mAxAgcSIzAgcQIHEiMwIHECBxIjMCBxAgcSKzKyiZf+so/tX37nhpP/V++9fmr5+Tf6cKx3uquR6hi0aYdXY/wf4M9OJAYgQOJETiQGIEDiRE4kBiBA4kROJAYgQOJETiQWJuVTR6yfcT263MxEIBy2uzBfy5pfeU5AFTQN/CIeEnS3+dgFgCF8R4cSKxY4LZ7tidtTx792+lSTwtgCMUCn7100eWXjZV6WgBD4BAdSKzNr8kek7RD0nLb07a/X38sACW0WZts41wMAqA8DtGBxAgcSIzAgcQIHEiMwIHECBxIjMCBxAgcSKzK0kVvvrYw9ZJCwHzBHhxIjMCBxAgcSIzAgcQIHEiMwIHECBxIjMCBxAgcSIzAgcTa3HRxie0XbO+zvdf2nXMxGIDhtbkW/ZSkH0XEbtsXS9pl+7mI2Fd5NgBDarM22dsRsbv5/H1JU5IW1x4MwPAG+msy20slrZC08yzf60nqSdK4FhYYDcCwWp9ks32RpCck3RUR733y+7OXLlqgC0rOCOActQrc9gLNxL0lIp6sOxKAUtqcRbekByVNRcR99UcCUEqbPfgaSbdKWmt7T/Px7cpzASigzdpkL0vyHMwCoDCuZAMSI3AgMQIHEiNwIDECBxIjcCAxAgcSI3AgsSprk82lY73VXY+AAWX+mS3atKPrET6GPTiQGIEDiRE4kBiBA4kROJAYgQOJETiQGIEDiRE4kFibmy6O2/6D7T82Sxf9ZC4GAzC8NpeqnpC0NiI+aG6f/LLtX0fE7yvPBmBIbW66GJI+aB4uaD6i5lAAymi78MGY7T2Sjkh6LiLOunSR7Unbkx/qROk5AZyDVoFHxOmIuFrShKRVtr98lm1YuggYMQOdRY+IdyW9IGl9nXEAlNTmLPrlti9tPv+0pOsk7a89GIDhtTmLfoWkR2yPaeZ/CL+MiGfqjgWghDZn0V/TzJrgAOYZrmQDEiNwIDECBxIjcCAxAgcSI3AgMQIHEiNwIDECBxIjcCAxAgcSI3AgMQIHEiNwIDECBxIjcCAxAgcSI3AgsdaBN/dGf9U292MD5olB9uB3SpqqNQiA8tqubDIh6QZJm+uOA6Cktnvw+yXdLemjirMAKKzNwgc3SjoSEbv6bMfaZMCIabMHXyPpJttvSXpc0lrbj35yI9YmA0ZP38Aj4t6ImIiIpZI2SHo+Im6pPhmAofF7cCCxNmuT/UdEvCjpxSqTACiOPTiQGIEDiRE4kBiBA4kROJAYgQOJETiQGIEDiQ10ocsoWrRpR9cjVHOst7rrEarI/DMbNezBgcQIHEiMwIHECBxIjMCBxAgcSIzAgcQIHEiMwIHEWl3J1txR9X1JpyWdioiVNYcCUMYgl6p+IyKOVZsEQHEcogOJtQ08JP3G9i7bvZoDASin7SH61yPisO3PSnrO9v6IeGn2Bk34PUka18LCYwI4F6324BFxuPnvEUlPSVp1lm1YuggYMW0WH7zQ9sVnPpf0TUmv1x4MwPDaHKJ/TtJTts9s/4uIeLbqVACK6Bt4RByU9NU5mAVAYfyaDEiMwIHECBxIjMCBxAgcSIzAgcQIHEiMwIHECBxIjMCBxAgcSIzAgcQIHEiMwIHECBxIjMCBxAgcSIzAgcRaBW77Uttbbe+3PWV7de3BAAyv7X3Rfyrp2Yj4ru3zJW58DswHfQO3fYmkayV9T5Ii4qSkk3XHAlBCm0P0ZZKOSnrY9qu2Nzf3Rwcw4toEfp6kayQ9EBErJB2XdM8nN7Ldsz1pe/JDnSg8JoBz0SbwaUnTEbGzebxVM8F/DEsXAaOnb+AR8Y6kQ7aXN19aJ2lf1akAFNH2LPodkrY0Z9APSrqt3kgASmkVeETskbSy8iwACuNKNiAxAgcSI3AgMQIHEiNwIDECBxIjcCAxAgcSI3AgsbaXqqIDizbt6HoEDOgf2744J//O6R+83Go79uBAYgQOJEbgQGIEDiRG4EBiBA4kRuBAYgQOJEbgQGJ9A7e93PaeWR/v2b5rLoYDMJy+l6pGxBuSrpYk22OSDkt6qvJcAAoY9BB9naQ/R8RfagwDoKxB/9hkg6THzvYN2z1JPUkaZ/FRYCS03oM3ix7cJOlXZ/s+SxcBo2eQQ/TrJe2OiL/WGgZAWYMEvlH/4/AcwGhqFXizHvh1kp6sOw6AktquTXZc0mWVZwFQGFeyAYkROJAYgQOJETiQGIEDiRE4kBiBA4kROJCYI6L8k9pHJQ36J6WLJB0rPsxoyPraeF3d+UJEXN5voyqBnwvbkxGxsus5asj62nhdo49DdCAxAgcSG6XAN3U9QEVZXxuva8SNzHtwAOWN0h4cQGEjEbjt9bbfsH3A9j1dz1OC7SW2X7C9z/Ze23d2PVNJtsdsv2r7ma5nKcn2pba32t5ve8r26q5nGkbnh+jNvdbf1MwdY6YlvSJpY0Ts63SwIdm+QtIVEbHb9sWSdkn6znx/XWfY/qGklZI+ExE3dj1PKbYfkfS7iNjc3Gh0YUS82/Vc52oU9uCrJB2IiIMRcVLS45Ju7nimoUXE2xGxu/n8fUlTkhZ3O1UZtick3SBpc9ezlGT7EknXSnpQkiLi5HyOWxqNwBdLOjTr8bSShHCG7aWSVkja2e0kxdwv6W5JH3U9SGHLJB2V9HDz9mNzcz/CeWsUAk/N9kWSnpB0V0S81/U8w7J9o6QjEbGr61kqOE/SNZIeiIgVko5LmtfnhEYh8MOSlsx6PNF8bd6zvUAzcW+JiCx3pF0j6Sbbb2nm7dRa2492O1Ix05KmI+LMkdZWzQQ/b41C4K9IutL2suakxgZJT3c809BsWzPv5aYi4r6u5yklIu6NiImIWKqZn9XzEXFLx2MVERHvSDpke3nzpXWS5vVJ0UHXJisuIk7Zvl3Sdkljkh6KiL0dj1XCGkm3SvqT7T3N134cEds6nAn93SFpS7OzOSjpto7nGUrnvyYDUM8oHKIDqITAgcQIHEiMwIHECBxIjMCBxAgcSIzAgcT+DYCdc44cs012AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = grid_world()\n",
    "#plt.ion()\n",
    "for ep in range(100):\n",
    "    action = np.random.randint(0,4)\n",
    "    env.step(action)\n",
    "    map = env.render()\n",
    "    plt.imshow(map)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
