{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## Tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A gridworld environment\n",
    "\n",
    "In order to build some intuition about RL methods, we are going to begin with a value-based method: Q-learning. To keep things simple we are going to implement Q-learning in its tabular form (i.e. without any function approximation).\n",
    "\n",
    "The first environment we are going to study is a discrete-space grid-world. We begin by importing all the required python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                      # numpy is for math operations.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt                         # matplotlib.pyplot is to make plots.\n",
    "from envs.environments import grid_world                # The grid_world environment.\n",
    "from agents.Qlearning import tabular_Qlearning          # The RL agent (tabular Q-learning).\n",
    "from utilities import plot_policies, plot_trayectory    # Pre-made functions to plot policies and trayectories.\n",
    "from utilities import print_actions_taken               # Pre-made function to print a log with the actions taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an instance of the environment, which we call \"env\", and we render the environment to see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAADFCAYAAAAG5C2JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAAo9JREFUeJzt3bFNxEAQQNFvRAlExJCTUgGdUBQtkJNQAK0QkNDALSWckGztre692LJXlr4msKzZxhjBtbuZfQC4BEKAhACVEKASAlRCgEoIUAkBKiFAVbdH3PT0/bjr5+qX+6c9b8eV+Ty9b+euMREgIUAlBKiEAJUQoBICVEKASghQCQEqIUAlBKiEAJUQoBICVEKASghQCQEqIUB10K+a1/hr5c/r8+wjnHX39jX7CBfLRICEAJUQoBICVEKASghQCQEqIUAlBKiEAJUQoBICVEKASghQCQEqIUAlBKiEAJUQoBICVEKASghQCQEqIUAlBKiEAJUQoBICVEKASghQCQEqIUAlBKiEAJUQoBICVEKASghQCQEqIUB10MLxva2wzHsFK7zHWUvRTQRICFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUiywcn7WE+j8s816biQAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBqkYXjK7DMex+/Hw9TnmsiQEKASghQCQEqIUAlBKiEAJUQoBICVEKASghQCQEqIUAlBKiEAJUQoBICVEKASghQCQGq2sYYs88A05kIkBCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQCUEqIQAlRCgEgJUQoBKCFAJASohQFV/xZ8dXrIbqvUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = grid_world()                  # Creates an environment instance called \"env\".\n",
    "env_map = env.get_map()             # Generates an image with the map of the environment.\n",
    "\n",
    "plt.figure(figsize=(3,3));          # These 3 are matplotplib functions that display the map of the environment.\n",
    "plt.imshow(env_map);                #\n",
    "plt.axis('off');                    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent (yellow) is going to be trained to achieve the goal cell (green). The obstacles (blue) are places where the agent cannot move to.\n",
    "\n",
    "The agent can execute 4 actions: 0-UP, 1-RIGHT, 2-DOWN, 3-LEFT. Each action moves the agent one cell in the corresponding direction. If the agent attemps to move towards an obstacle or a limit of the gridworld, it will remain at its current position.\n",
    "\n",
    "To interact with the environment object \"env\", we are going to call the following functions:\n",
    "\n",
    "### env.step(): \n",
    "* **Input:** action. \n",
    "* **Output:** next state, reward and done (boolean flag to indicate terminal state). \n",
    "* **Function:** move the agent one cell in the direction given by the action (i.e. 0-UP, 1-RIGHT, 2-DOWN, 3-LEFT)\n",
    "\n",
    "### env.reset():\n",
    "* **Input:** a boolean flag to start at random positions (optional).\n",
    "* **Output:** state. \n",
    "* **Function:** resets the environment.\n",
    "\n",
    "### env.get_map():\n",
    "* **Input:** none. \n",
    "* **Output:** the current map of the environment. \n",
    "* **Function:** return the map of the environment for rendering purposes.\n",
    "\n",
    "As an example, lets run the environment for 10 steps taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1    Action taken: RIGHT\n",
      "Step: 2    Action taken: DOWN\n",
      "Step: 3    Action taken: LEFT\n",
      "Step: 4    Action taken: RIGHT\n",
      "Step: 5    Action taken: LEFT\n",
      "Step: 6    Action taken: LEFT\n",
      "Step: 7    Action taken: UP\n",
      "Step: 8    Action taken: UP\n",
      "Step: 9    Action taken: LEFT\n",
      "Step: 10    Action taken: LEFT\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = grid_world()                  # Creates an environment instance called \"env\".\n",
    "env.reset()                         # Resets the environment.\n",
    "actions = []                        # Creates an empty python list to save the actions taken.\n",
    "trayectory = []                     # Creates an empty python list to save the trayectory.\n",
    "for i in range(10):\n",
    "    action = np.random.randint(0,4) # Generates a random int between 0 and 3 (4-1).\n",
    "    env.step(action)                # Takes the action (moves the agent). \n",
    "    env_map = env.get_map()         # Returns the current map of the environment\n",
    "    \n",
    "    trayectory.append(env_map)      # Saves the current map in the trayectory list.\n",
    "    actions.append(action)          # Saves the action taken in the actions list.\n",
    "    \n",
    "\n",
    "print_actions_taken(actions)            # Prints a log with the actions taken at each step.\n",
    "plot_trayectory(trayectory,size=(10,5)) # Plots the trayectory step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train the agent?\n",
    "\n",
    "As we mentioned before, the agent (yellow) is going to be trained to acheieve the goal cell (green). To train it we are going to use the tabular version of the Q-learning algorithm:\n",
    "\n",
    "![title](imgs/Qlearning_sutton131.png)\n",
    "\n",
    "Image taken from page 131 of: **Sutton, R. S., & Barto, A. G. (2018). Introduction to reinforcement learning. Cambridge: MIT press**.\n",
    "\n",
    "#### A Markov Decision Process\n",
    "\n",
    "Recall that a Markov Decision Process (MDP) is a tuple of the form: $(S,A,T,R)$, where:\n",
    "* $S$ denotes the set of states of the environment.\n",
    "* $A$ denotes the set of actions that the agent can execute.\n",
    "* $T$ denotes the state-transition probabilities of the environment.\n",
    "* $R$ denotes the reward function that the agent recieves from the environment.\n",
    "\n",
    "In our especific example with the grid_world environment:\n",
    "* $S$ is the set of cells of the grid_world enviroment. In our example $|S|=64$ because the grid_world has dimensions of $8\\times8$\n",
    "* $A$ is the set (0,1,2,3) of actions that the agent can execute (moving up, left, down or right).\n",
    "* $T$ is the transition function from one cell to another given an action. In our case this is a deterministic function of the environment and is unknown to the agent.\n",
    "* $R$ is a scalar function that has to reinforce the desired behavior of the agent. We are going to define this function later.\n",
    "\n",
    "\n",
    "#### The Q-learning algorithm explained\n",
    "\n",
    "The Q-learning algorithm is an iterative algorithm that seeks to learn the \"Quality\" function $Q(s,a)$. This function maps tuples $(s,a)$ to a real scalar value (i.e. $S\\times A\\rightarrow \\mathbb{R}$). This value, usually called Q-value, can be seen as the quality of being at a state $s\\in S$ and taking an action $a\\in A$.\n",
    "\n",
    "Applying Bellman's principle of optimality, the function Q(s,a) can be recursively expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    Q(s_t,a_t) = r_{t+1} + Q(s_{t+1},a_{t+1})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### POR COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent class\n",
    "\n",
    "In order to save some time and focus on the fundamentals of Q-learning, we have already coded an agent class in python.\n",
    "\n",
    "The agent class initializes a Q-table with $|S|$ rows and $|A|$ columns and all entries in $0$ and has the following methods already implemented:\n",
    "\n",
    "* greedy(): takes an state as input and returns an action being greedy with the current Q-values.\n",
    "* epsilon_greedy(): takes an state as input and returns a random action with epsilon probability or a greedy action with (1-epsilon) probability.\n",
    "* epsilon_decay(): exponentially decays the exploration parameter epsilon (i.e. epsilon = decay$*$epsilon with decay<1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm (main code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Qlearning(env,agent,max_episodes,max_steps,exp_decay,start=False,seed=1234):\n",
    "    np.random.seed(seed)                                           # Fixed random seed for reproducibility.\n",
    "    policies = []                                                  # Creates an empty python list to save the policies.\n",
    "    \n",
    "    for episode in range(max_episodes+1):                          # Iterates over the number of episodes.\n",
    "        state = env.reset(start)                                   # Resets the environment.\n",
    "        done = False                                               # The done flag starts as False.\n",
    "        step = 1                                                   # Step counter starts at 1.\n",
    "        policies.append(agent.get_policy())                        # Adds current policy to the policies list.\n",
    "        \n",
    "        while (not done and step <= max_steps):                    # While loop for current episode.\n",
    "            action = agent.epsilon_greedy(state)                   # Agent selects an action with epsilon-greedy policy.\n",
    "            next_state,reward,done = env.step(action)              # Agent takes the selected action.\n",
    "            agent.train(state,action,next_state,reward,done)       # Agent trains (updates Q-table entry).\n",
    "            \n",
    "            state = next_state                                     # Updates the current state.\n",
    "            step += 1                                              # Increments the step counter in 1.\n",
    "            \n",
    "        agent.epsilon_decay(rate=exp_decay,min=0.1)                # Reduces the exploration parameter epsilon.\n",
    "        \n",
    "        if(episode%(max_episodes/10)==0):                          # This prints some information of the training process\n",
    "            print('Episode: ',episode,' Steps: ',step,\n",
    "                  ' Exploration: ',np.round(agent._epsilon,2))\n",
    "            \n",
    "    return policies                                                # The function returns the policies list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the agent.train() function\n",
    "\n",
    "In order to understand Q-learning, we are going to implement the train function of our tabular_Qlearning agent.\n",
    "\n",
    "This function performs the update:\n",
    "\\begin{equation}\n",
    "    Q(s_t,a_t) = Q(s_t,a_t) + \\alpha\\bigg[r_{t+1} + \\gamma\\max_{\\hat{a}}Q(s_{t+1},\\hat{a}) - Q(s_t,a_t) \\bigg]  \n",
    "\\end{equation}\n",
    "\n",
    "To implement only this function we are going to create an instance of the tabular_Qlearning class and override the train method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class our_Qlearning(tabular_Qlearning):          # This is a subclass of tabular_Qlearning and is called: our_Qlearning.\n",
    "    \n",
    "    def train(self,state,action,next_state,reward,done):  # Takes as input: state, action, next_state, reward, and done.\n",
    "        if (not done):                                    # If the next_state is not a terminal state.\n",
    "            BE = reward + self._gamma*np.amax(self.Qtable[next_state,:]) - self.Qtable[state,action] # Bellman error.\n",
    "        else:\n",
    "            BE = reward - self.Qtable[state,action] # Bellman error.\n",
    "        self.Qtable[state,action] += self._alpha*BE # Table update (alpha denotes the learning rate).\n",
    "        return 0.5*np.square(BE)                    # Returns half of the squared Bellman error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the reward function\n",
    "\n",
    "As we mentioned before, the reward function of the environment is a scalar function that has to reinforce the desired behavior of the agent. In our case we want the agent to achieve the goal cell. To do so we are going to define two different reward functions:\n",
    "\n",
    "#### Reward 1\n",
    "\\begin{equation}\n",
    "    R(s) = \n",
    "        \\begin{cases}\n",
    "        +1, \\text{    if the agent's cell is the goal cell.}\\\\\n",
    "        0, \\text{    otherwise.}\n",
    "        \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "#### Reward 2\n",
    "\\begin{equation}\n",
    "    R(s) = \n",
    "        \\begin{cases}\n",
    "        +1, \\text{    if the agent's cell is the goal cell.}\\\\\n",
    "        -1, \\text{    otherwise.}\n",
    "        \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Notice that both rewards reinforce the behavior of moving towards the goal cell. However, the second reward gives a penalty for every state before the goal is achieved. As we will see, this penalty encourages the agent to achieve the goal as fast as possible.\n",
    "\n",
    "In order to define these two reward functions, we are going to create two subclasses of the environment class \"grid_world\", and define a different reward function in each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_world_v1(grid_world):        # The first subclass is called: grid_world_v1.\n",
    "\n",
    "    def reward(self):\n",
    "        agent, target = self.observe()  # This function belongs to the grid_world class.\n",
    "        if agent==target:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "class grid_world_v2(grid_world):        # The second subclass is called: grid_world_v2.       \n",
    "    \n",
    "    def reward(self):\n",
    "        agent, target = self.observe()  # This function belongs to the grid_world class.\n",
    "        if agent==target:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets run some experiments!\n",
    "\n",
    "In order to explore the Q-learning algorithm lets run some experiments. For each experiment, set the parameters of the **Train the agent** cell as shown and then try to answer the question. After training, run the **Test the agent** cell to see the behavior learned by the agent.\n",
    "\n",
    "#### Experiment 1\n",
    "* env = grid_world_v1()\n",
    "* max_eps = 100\n",
    "* max_steps = 100\n",
    "* exp_decay = 0.99\n",
    "* random = False\n",
    "\n",
    "**Why does the agent fail to solve the environment?**\n",
    "\n",
    "#### Experiment 2\n",
    "* env = grid_world_v1()\n",
    "* max_eps = 100\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.99\n",
    "* random = False\n",
    "\n",
    "**Can the agent solve the environment from every possible state?**\n",
    "\n",
    "#### Experiment 3\n",
    "* env = grid_world_v1()\n",
    "* max_eps = 500\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.99\n",
    "* random = True\n",
    "\n",
    "**Is the final policy optimal?**\n",
    "\n",
    "#### Experiment 4\n",
    "* env = grid_world_v1()\n",
    "* max_eps = 500\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.90\n",
    "* random = True\n",
    "\n",
    "**Why does the agent fail to solve the environment now?**\n",
    "\n",
    "#### Experiment 5\n",
    "* env = grid_world_v2()\n",
    "* max_eps = 500\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.99\n",
    "* random = True\n",
    "\n",
    "**Is the final policy better than the one of experiment 3? Is it optimal?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  Steps:  101  Exploration:  0.99\n",
      "Episode:  10  Steps:  101  Exploration:  0.9\n",
      "Episode:  20  Steps:  101  Exploration:  0.81\n",
      "Episode:  30  Steps:  101  Exploration:  0.73\n",
      "Episode:  40  Steps:  101  Exploration:  0.66\n",
      "Episode:  50  Steps:  101  Exploration:  0.6\n",
      "Episode:  60  Steps:  101  Exploration:  0.54\n",
      "Episode:  70  Steps:  101  Exploration:  0.49\n",
      "Episode:  80  Steps:  101  Exploration:  0.44\n",
      "Episode:  90  Steps:  101  Exploration:  0.4\n",
      "Episode:  100  Steps:  101  Exploration:  0.36\n",
      "______________________\n",
      "UP, RIGHT, DOWN, LEFT\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAAA/CAYAAABn/8O7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAAPRJREFUeJzt3TERAjEQQNG7G/CADFoE0NNgAXUIwQMSwMOhAIaG/IL36szsFn/SpMi8rusElaVegP8mQFICJCVAUgIkJUBSAiQlQFICJCVAUpt6gXeOy3noG+Hzchg5btqeHkPn3fbXofOW3X3+6tyvF4FPBEhKgKQESEqApARISoCkBEhKgKQESEqApARISoCkBEhKgKQESEqApARISoCkBEhKgKQESEqApARISoCkBEhKgKRm3zRQcgOSEiApAZISICkBkhIgKQGSEiApAZISICkBkhIgKQGSEiApAZISICkBkhIgKQGSEiApAZISICkBknoBGV4NeR+MBwcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = grid_world_v1()                                              # Environment instance with the first reward function.\n",
    "max_eps = 100                                                      # Max number of episodes to train\n",
    "max_steps = 100                                                    # Max number of steps per episode during training\n",
    "exp_decay = 0.99                                                   # Exploration parameter decay\n",
    "random = False                                                     # Flag to use random initial state\n",
    "\n",
    "agent = our_Qlearning(num_states=64,num_actions=4,lr=0.5)          # Agent as instance of our_Qlearning class.\n",
    "\n",
    "policies = run_Qlearning(env, agent, max_eps, max_steps, exp_decay, random) # Run the Qlearning algorithm\n",
    "\n",
    "plot_policies(policies,(1,1))                         # Plot some policies (add (1,1) arg to plot only the last one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the agent \n",
    "#### (runs the learned policy for 1 episode of 20 steps max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAJCCAYAAAD+/jc0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADqtJREFUeJzt2bGN20AChtHbA0tQtLGdb6oKthMVpRacO3EBasWBEjewuvCATw4Ig+SQwnu5OCPgTz7M2+Px+A8AAAD/99/RFwAAANgboQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIhpjY9+/f7++Nfffr5/LHkVFvbr68fb6DuswWZfl80+s9l9s9lnNrtvNvvMZvdt7ma9KAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATGt89PP9Y43Prup+OW9+5ul62/xM/s5m57HZ/bDZeWx2P2x2HpvdD5ud55U360UJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABiGn2Bul/Oo6+wmVH/9XS9DTn3Vdns+mx2WTa7Pptdls2uz2aXZbPr22KzXpQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQ0+gL1Ol6G3Lu/XLe/MxR/5Vl2SxHY7Mcjc1yNDb7GrwoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATKMvsBen6230FTbz5+e30VdgATbL0dgsR2OzHI3NLsuLEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIi3x+Mx+g4AAAC74kUJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIhpjY9+/f7++Nfffr5/LHkVFvbr68fb6DuswWZfl80+s9l9s9lnNrtvNvvMZvdt7ma9KAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATGt89PP9Y43Prup+OW9+5ul62/xM/s5m57HZ/bDZeWx2P2x2HpvdD5ud55U360UJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABiGn2Bul/Oo6+wmVH/9XS9DTn3Vdns+mx2WTa7Pptdls2uz2aXZbPr22KzXpQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQ0+gL1Ol6G3Lu/XLe/MxR/5Vl2SxHY7Mcjc1yNDb7GrwoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATKMvsBen6230FTbz5+e30VdgATbL0dgsR2OzHI3NLsuLEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIi3x+Mx+g4AAAC74kUJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIhpjY9+/f7++Nfffr5/LHkVFvbr68fb6DuswWZfl80+s9l9s9lnNrtvNvvMZvdt7ma9KAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATGt89PP9Y43Prup+OW9+5ul62/xM/s5m57HZ/bDZeWx2P2x2HpvdD5ud55U360UJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABiGn2Bul/Oo6+wmVH/9XS9DTn3Vdns+mx2WTa7Pptdls2uz2aXZbPr22KzXpQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQ0+gL1Ol6G3Lu/XLe/MxR/5Vl2SxHY7Mcjc1yNDb7GrwoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATKMvsBen6230FTbz5+e30VdgATbL0dgsR2OzHI3NLsuLEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIi3x+Mx+g4AAAC74kUJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIhpjY9+/f7++Nfffr5/LHkVFvbr68fb6DuswWZfl80+s9l9s9lnNrtvNvvMZvdt7ma9KAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATGt89PP9Y43Prup+OW9+5ul62/xM/s5m57HZ/bDZeWx2P2x2HpvdD5ud55U360UJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABiGn2Bul/Oo6+wmVH/9XS9DTn3Vdns+mx2WTa7Pptdls2uz2aXZbPr22KzXpQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQ0+gL1Ol6G3Lu/XLe/MxR/5Vl2SxHY7Mcjc1yNDb7GrwoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAAihBAAAEEIJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABATKMvsBen6230FTbz5+e30VdgATbL0dgsR2OzHI3NLsuLEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIi3x+Mx+g4AAAC74kUJAAAghBIAAEAIJQAAgBBKAAAAIZQAAABCKAEAAIRQAgAACKEEAAAQQgkAACCEEgAAQAglAACAEEoAAAAhlAAAAEIoAQAAhFACAAAIoQQAABBCCQAAIIQSAABACCUAAIAQSgAAACGUAAAAQigBAACEUAIAAIj/AVpsx/z6lhQtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trayectory = []                            # Create empty python list to save the trayectory.\n",
    "state = env.reset(random)                  # Reset the environment.\n",
    "trayectory.append(env.get_map())           # Save the current (initial) map in the list.\n",
    "done = False                               # done flag starts as false.\n",
    "step = 1                                   # step counter starts in 1.\n",
    "while (not done and step < 20):            # Looping until terminal or during max steps.\n",
    "    action = agent.greedy(state)           # Select action based on greedy policy.\n",
    "    state,_,done = env.step(action)        # Take selected action.\n",
    "    step += 1                              # Increase step counter by one.\n",
    "\n",
    "    trayectory.append(env.get_map())       # Save the current map in the list.\n",
    "        \n",
    "plot_trayectory(trayectory)                # Plot the taken trayectory step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding remarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
